{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1648cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TTA에 필요한 cv2 임포트\n",
    "import cv2\n",
    "# Softmax를 위한 torch.nn.functional 임포트\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Weights & Biases (wandb) 임포트\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cb3bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 시드를 고정합니다. (추론 시에도 재현성을 위해)\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d8ce8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터셋 클래스를 정의합니다. (training.py와 동일하게 유지)\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, path, transform=None, return_raw=False):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.return_raw = return_raw\n",
    "\n",
    "        if 'ID' not in self.df.columns:\n",
    "            raise ValueError(f\"CSV 파일 '{csv_file}'에 'ID' 컬럼이 없습니다.\")\n",
    "        if 'target' not in self.df.columns:\n",
    "            # 테스트 CSV 파일은 'target' 컬럼이 없을 수 있으므로 경고만 표시\n",
    "            print(f\"경고: CSV 파일 '{csv_file}'에 'target' 컬럼이 없습니다. 테스트 데이터셋으로 가정합니다.\", flush=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.loc[idx, 'ID']\n",
    "        # 테스트셋의 경우 'target' 컬럼이 없을 수 있으므로 기본값 0 사용\n",
    "        target = self.df.loc[idx, 'target'] if 'target' in self.df.columns else 0 \n",
    "\n",
    "        img_path = os.path.join(self.path, img_name)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"경고: 이미지 파일이 존재하지 않습니다: {img_path}. 이 항목은 건너뛰거나 에러가 발생할 수 있습니다.\", flush=True)\n",
    "\n",
    "        if self.return_raw:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "            return img, target\n",
    "        else:\n",
    "            if self.transform:\n",
    "                if hasattr(self.transform, '__module__') and 'torchvision.transforms' in self.transform.__module__:\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    img = self.transform(img)\n",
    "                else:\n",
    "                    img = np.array(Image.open(img_path).convert('RGB'))\n",
    "                    img = self.transform(image=img)['image']\n",
    "            else:\n",
    "                img = np.array(Image.open(img_path).convert('RGB'))\n",
    "\n",
    "            return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TTA를 위한 추론 함수\n",
    "# def inference_with_tta(loader, model, device, tta_augments, base_transform):\n",
    "#     model.eval()\n",
    "#     all_preds_proba = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         pbar = tqdm(loader)\n",
    "#         for raw_image_batch, _ in pbar:\n",
    "#             raw_image = raw_image_batch.squeeze(0).cpu().numpy()\n",
    "\n",
    "#             single_image_tta_preds = []\n",
    "\n",
    "#             for tta_aug in tta_augments:\n",
    "#                 augmented_img_np = tta_aug(image=raw_image)['image']\n",
    "#                 transformed_img_tensor = base_transform(image=augmented_img_np)['image']\n",
    "#                 transformed_img_tensor = transformed_img_tensor.unsqueeze(0).to(device)\n",
    "#                 output = model(transformed_img_tensor)\n",
    "#                 prob = F.softmax(output, dim=1).cpu().numpy()\n",
    "#                 single_image_tta_preds.append(prob)\n",
    "\n",
    "#             avg_prob = np.mean(single_image_tta_preds, axis=0)\n",
    "#             all_preds_proba.append(avg_prob)\n",
    "\n",
    "#     final_preds_proba = np.concatenate(all_preds_proba, axis=0)\n",
    "\n",
    "#     return final_preds_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TTA를 위한 추론 함수\n",
    "def inference_with_tta(loader, model, device, tta_augments, base_transform):\n",
    "    model.eval()\n",
    "    all_preds_proba = []\n",
    "    all_img_names = [] # 이미지 이름을 저장\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader)\n",
    "        for raw_image_batch, img_names_batch in pbar: # 이미지 이름도 받음\n",
    "            raw_image = raw_image_batch.squeeze(0).cpu().numpy()\n",
    "            img_name = img_names_batch[0] # 배치 크기 1이므로 첫 번째 요소\n",
    "\n",
    "            single_image_tta_preds = []\n",
    "\n",
    "            for tta_aug in tta_augments:\n",
    "                augmented_img_np = tta_aug(image=raw_image)['image']\n",
    "                transformed_img_tensor = base_transform(image=augmented_img_np)['image']\n",
    "                transformed_img_tensor = transformed_img_tensor.unsqueeze(0).to(device)\n",
    "                output = model(transformed_img_tensor)\n",
    "                prob = F.softmax(output, dim=1).cpu().numpy()\n",
    "                single_image_tta_preds.append(prob)\n",
    "\n",
    "            avg_prob = np.mean(single_image_tta_preds, axis=0)\n",
    "            all_preds_proba.append(avg_prob)\n",
    "            all_img_names.append(img_name) # 이미지 이름 저장\n",
    "\n",
    "    final_preds_proba = np.concatenate(all_preds_proba, axis=0)\n",
    "\n",
    "    return final_preds_proba, all_img_names # 확률과 이미지 이름 반환\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa309d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Hyper-parameters (추론에 필요한 최소한의 파라미터) ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "test_csv = \"data/sample_submission.csv\"\n",
    "test_path = \"data/test/\"\n",
    "checkpoint_dir = \"checkpoints\" # 학습된 모델이 저장된 경로\n",
    "\n",
    "model_name = 'convnext_base' # 학습 시 사용한 모델 이름과 동일해야 함\n",
    "img_size = 224 # 학습 시 사용한 이미지 크기와 동일해야 함\n",
    "NUM_CLASSES = 17 # 분류할 클래스 개수 (학습 시와 동일해야 함)\n",
    "BATCH_SIZE = 32 # 일반 추론 시 배치 크기\n",
    "num_workers = 0 # DataLoader worker 수\n",
    "\n",
    "USE_TTA = True # TTA 사용 여부 플래그\n",
    "N_SPLITS = 5 # K-Fold 학습 시 사용한 폴드 개수와 동일해야 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Weights & Biases 초기화 (추론용) ---\n",
    "# wandb.init(project=\"document-type-classification-inference\",\n",
    "#            name=f\"inference_{model_name}_k{N_SPLITS}_tta{USE_TTA}\", # 추론 실행 이름\n",
    "#            config={\n",
    "#                \"model_name\": model_name,\n",
    "#                \"img_size\": img_size,\n",
    "#                \"num_classes\": NUM_CLASSES,\n",
    "#                \"use_tta\": USE_TTA,\n",
    "#                \"n_splits\": N_SPLITS, # K-Fold 정보 추가\n",
    "#            },\n",
    "#            mode=\"online\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89795c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Data Transforms (추론에 필요한 전처리) ---\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# TTA specific augmentations\n",
    "tta_augments = [\n",
    "    A.NoOp(),\n",
    "    A.HorizontalFlip(p=1),\n",
    "    A.VerticalFlip(p=1),\n",
    "    A.Rotate(limit=90, p=1, interpolation=cv2.INTER_LINEAR),\n",
    "    A.Rotate(limit=180, p=1, interpolation=cv2.INTER_LINEAR),\n",
    "    A.Rotate(limit=270, p=1, interpolation=cv2.INTER_LINEAR),\n",
    "    # A.RandomBrightnessContrast(p=1, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n",
    "    # A.RandomGamma(p=1, gamma_limit=(80, 120)),\n",
    "    # A.GaussNoise(p=1, var_limit=(10.0, 50.0)),\n",
    "    A.Sharpen(alpha=(0.3, 0.6), lightness=(0.7, 1.0), p=1.0),\n",
    "    A.CLAHE(p=1, clip_limit=2.0, tile_grid_size=(8,8)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95f4a1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting inference process...\n",
      "\n",
      "--- Loading model for Fold 1/5 ---\n",
      "Model loaded from checkpoints/best_model_fold_0.pth\n",
      "Performing inference with Test-Time Augmentation (TTA) for Fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3140/3140 [03:52<00:00, 13.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Fold 1 completed.\n",
      "\n",
      "--- Loading model for Fold 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoints/best_model_fold_1.pth\n",
      "Performing inference with Test-Time Augmentation (TTA) for Fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3140/3140 [03:53<00:00, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Fold 2 completed.\n",
      "\n",
      "--- Loading model for Fold 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoints/best_model_fold_2.pth\n",
      "Performing inference with Test-Time Augmentation (TTA) for Fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3140/3140 [03:52<00:00, 13.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Fold 3 completed.\n",
      "\n",
      "--- Loading model for Fold 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoints/best_model_fold_3.pth\n",
      "Performing inference with Test-Time Augmentation (TTA) for Fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3140/3140 [03:51<00:00, 13.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Fold 4 completed.\n",
      "\n",
      "--- Loading model for Fold 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoints/best_model_fold_4.pth\n",
      "Performing inference with Test-Time Augmentation (TTA) for Fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3140/3140 [03:51<00:00, 13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Fold 5 completed.\n",
      "\n",
      "--- Averaging predictions from all folds ---\n",
      "Final ensemble predictions saved to submission_kfold_ensemble_tta.csv\n",
      "============================================================\n",
      "🎉 All inference processes completed! 🎉\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Inference & Save File ---\n",
    "print(\"🚀 Starting inference process...\", flush=True)\n",
    "\n",
    "# 모든 폴드의 예측 확률을 저장할 리스트\n",
    "all_fold_predictions_proba = []\n",
    "\n",
    "# K-Fold 모델 로드 및 예측 수행\n",
    "for fold in range(N_SPLITS):\n",
    "    fold_model_path = os.path.join(checkpoint_dir, f'best_model_fold_{fold}.pth')\n",
    "    \n",
    "    if not os.path.exists(fold_model_path):\n",
    "        print(f\"Error: Model for Fold {fold} not found at {fold_model_path}. Skipping this fold.\", flush=True)\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Loading model for Fold {fold+1}/{N_SPLITS} ---\", flush=True)\n",
    "    model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=False,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(device)\n",
    "    model.load_state_dict(torch.load(fold_model_path)['model_state_dict'])\n",
    "    model.to(device)\n",
    "    print(f\"Model loaded from {fold_model_path}\", flush=True)\n",
    "\n",
    "    if USE_TTA:\n",
    "        print(f\"Performing inference with Test-Time Augmentation (TTA) for Fold {fold+1}...\", flush=True)\n",
    "        tta_test_dataset = ImageDataset(csv_file=test_csv, path=test_path, return_raw=True)\n",
    "        tta_test_loader = DataLoader(tta_test_dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        fold_preds_proba = inference_with_tta(\n",
    "            loader=tta_test_loader,\n",
    "            model=model,\n",
    "            device=device,\n",
    "            tta_augments=tta_augments,\n",
    "            base_transform=tst_transform\n",
    "        )\n",
    "        all_fold_predictions_proba.append(fold_preds_proba)\n",
    "        print(f\"Inference for Fold {fold+1} completed.\", flush=True)\n",
    "\n",
    "    else:\n",
    "        print(f\"Performing normal inference for Fold {fold+1}...\", flush=True)\n",
    "        test_dataset_normal_inference = ImageDataset(\n",
    "            csv_file=test_csv,\n",
    "            path=test_path,\n",
    "            transform=tst_transform,\n",
    "            return_raw=False\n",
    "        )\n",
    "        test_loader_normal = DataLoader(\n",
    "            test_dataset_normal_inference,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        fold_preds_proba_list = []\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(test_loader_normal)\n",
    "            for image, _ in pbar:\n",
    "                image = image.to(device)\n",
    "                outputs = model(image)\n",
    "                prob = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                fold_preds_proba_list.extend(prob)\n",
    "        all_fold_predictions_proba.append(np.array(fold_preds_proba_list))\n",
    "        print(f\"Inference for Fold {fold+1} completed.\", flush=True)\n",
    "\n",
    "# 모든 폴드의 예측 확률 평균\n",
    "if all_fold_predictions_proba:\n",
    "    print(\"\\n--- Averaging predictions from all folds ---\", flush=True)\n",
    "    final_avg_preds_proba = np.mean(all_fold_predictions_proba, axis=0)\n",
    "    final_preds = np.argmax(final_avg_preds_proba, axis=1)\n",
    "\n",
    "    submission_df = pd.read_csv(test_csv)\n",
    "    submission_file_name = 'submission_kfold_ensemble_tta.csv' if USE_TTA else 'submission_kfold_ensemble_normal.csv'\n",
    "    submission_df['target'] = final_preds\n",
    "    submission_df.to_csv(submission_file_name, index=False)\n",
    "    print(f\"Final ensemble predictions saved to {submission_file_name}\", flush=True)\n",
    "\n",
    "    # --- Weights & Biases에 제출 파일 아티팩트로 기록 ---\n",
    "    # artifact = wandb.Artifact(f'submission_ensemble_tta_{N_SPLITS}folds' if USE_TTA else f'submission_ensemble_normal_{N_SPLITS}folds', type='submission')\n",
    "    # artifact.add_file(submission_file_name)\n",
    "    # wandb.log_artifact(artifact)\n",
    "    # wandb.log({\"final_submission_file\": submission_file_name})\n",
    "\n",
    "else:\n",
    "    print(\"No models were successfully loaded or processed for inference. No submission file generated.\", flush=True)\n",
    "\n",
    "print(\"=\" * 60, flush=True)\n",
    "print(f\"🎉 All inference processes completed! 🎉\", flush=True)\n",
    "\n",
    "# --- Weights & Biases 실행 종료 ---\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b94f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo labeling 을 위한 inference 코드\n",
    "\n",
    "# TTA를 위한 추론 함수\n",
    "def inference_with_tta(loader, model, device, tta_augments, base_transform):\n",
    "    model.eval()\n",
    "    all_preds_proba = []\n",
    "    all_img_names = [] # 이미지 이름을 저장\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader)\n",
    "        for raw_image_batch, img_names_batch in pbar: # 이미지 이름도 받음\n",
    "            raw_image = raw_image_batch.squeeze(0).cpu().numpy()\n",
    "            img_name = img_names_batch[0] # 배치 크기 1이므로 첫 번째 요소\n",
    "\n",
    "            single_image_tta_preds = []\n",
    "\n",
    "            for tta_aug in tta_augments:\n",
    "                augmented_img_np = tta_aug(image=raw_image)['image']\n",
    "                transformed_img_tensor = base_transform(image=augmented_img_np)['image']\n",
    "                transformed_img_tensor = transformed_img_tensor.unsqueeze(0).to(device)\n",
    "                output = model(transformed_img_tensor)\n",
    "                prob = F.softmax(output, dim=1).cpu().numpy()\n",
    "                single_image_tta_preds.append(prob)\n",
    "\n",
    "            avg_prob = np.mean(single_image_tta_preds, axis=0)\n",
    "            all_preds_proba.append(avg_prob)\n",
    "            all_img_names.append(img_name) # 이미지 이름 저장\n",
    "\n",
    "    final_preds_proba = np.concatenate(all_preds_proba, axis=0)\n",
    "\n",
    "    return final_preds_proba, all_img_names # 확률과 이미지 이름 반환\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef61431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Hyper-parameters (추론에 필요한 최소한의 파라미터) ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "test_csv = \"data/sample_submission.csv\"\n",
    "test_path = \"data/test/\"\n",
    "# checkpoint_dir = \"checkpoints_0707_kfold_conv\" # 학습된 모델이 저장된 경로\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "\n",
    "model_name = 'convnext_base' # 학습 시 사용한 모델 이름과 동일해야 함\n",
    "img_size = 224 # 학습 시 사용한 이미지 크기와 동일해야 함\n",
    "NUM_CLASSES = 17 # 분류할 클래스 개수 (학습 시와 동일해야 함)\n",
    "BATCH_SIZE = 32 # 일반 추론 시 배치 크기\n",
    "num_workers = 0 # DataLoader worker 수\n",
    "\n",
    "USE_TTA = True # TTA 사용 여부 플래그\n",
    "N_SPLITS = 5 # K-Fold 학습 시 사용한 폴드 개수와 동일해야 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f93017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pseudo-labeling specific parameters\n",
    "CONFIDENCE_THRESHOLD = 0.9 # 의사 레이블을 부여할 확신도 임계값\n",
    "PSEUDO_LABEL_CSV_PATH = \"train_pseudo_labeled.csv\" # 의사 레이블링된 데이터 저장 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfa31646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting pseudo-labeling process...\n",
      "Loading 5 models and predicting on test data...\n",
      "--- Predicting with model from Fold 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3140/3140 [05:36<00:00,  9.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Fold 1 completed.\n",
      "--- Predicting with model from Fold 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3140/3140 [05:36<00:00,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Fold 2 completed.\n",
      "--- Predicting with model from Fold 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3140/3140 [05:38<00:00,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Fold 3 completed.\n",
      "--- Predicting with model from Fold 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3140/3140 [05:38<00:00,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Fold 4 completed.\n",
      "--- Predicting with model from Fold 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3140/3140 [05:42<00:00,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Fold 5 completed.\n",
      "\n",
      "--- Averaging predictions from all folds ---\n",
      "Filtering pseudo-labels with confidence threshold: 0.9\n",
      "Total test samples: 3140\n",
      "Number of pseudo-labeled samples: 1774\n",
      "Pseudo-labeled dataset saved to train_pseudo_labeled.csv\n",
      "============================================================\n",
      "🎉 Pseudo-labeling process completed! 🎉\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Data Transforms (추론에 필요한 전처리) ---\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# TTA specific augmentations\n",
    "tta_augments = [\n",
    "    A.NoOp(),\n",
    "    A.HorizontalFlip(p=1),\n",
    "    A.VerticalFlip(p=1),\n",
    "    A.Rotate(limit=90, p=1, interpolation=cv2.INTER_LINEAR),\n",
    "    A.Rotate(limit=180, p=1, interpolation=cv2.INTER_LINEAR),\n",
    "    A.Rotate(limit=270, p=1, interpolation=cv2.INTER_LINEAR),\n",
    "    A.RandomBrightnessContrast(p=1, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n",
    "    A.RandomGamma(p=1, gamma_limit=(80, 120)),\n",
    "    A.GaussNoise(p=1, var_limit=(10.0, 50.0)),\n",
    "    A.CLAHE(p=1, clip_limit=2.0, tile_grid_size=(8,8)),\n",
    "]\n",
    "\n",
    "# --- Main Pseudo-Labeling Process ---\n",
    "try:\n",
    "    print(\"🚀 Starting pseudo-labeling process...\", flush=True)\n",
    "\n",
    "    # 1. 테스트 데이터 로드\n",
    "    # ImageDataset에서 이미지 이름도 반환하도록 수정했으므로, return_raw=True를 사용\n",
    "    test_dataset = ImageDataset(csv_file=test_csv, path=test_path, return_raw=True)\n",
    "    # TTA를 위해 batch_size=1로 설정\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # 모든 폴드의 예측 확률을 저장할 리스트\n",
    "    all_fold_predictions_proba = []\n",
    "\n",
    "    # 2. K-Fold 모델 로드 및 테스트 데이터에 대한 예측 수행\n",
    "    print(f\"Loading {N_SPLITS} models and predicting on test data...\", flush=True)\n",
    "    for fold in range(N_SPLITS):\n",
    "        fold_model_path = os.path.join(checkpoint_dir, f'best_model_fold_{fold}.pth')\n",
    "        \n",
    "        if not os.path.exists(fold_model_path):\n",
    "            print(f\"경고: Fold {fold}의 모델을 찾을 수 없습니다: {fold_model_path}. 이 폴드는 건너뜁니다.\", flush=True)\n",
    "            continue\n",
    "\n",
    "        print(f\"--- Predicting with model from Fold {fold+1}/{N_SPLITS} ---\", flush=True)\n",
    "        model = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=False,\n",
    "            num_classes=NUM_CLASSES\n",
    "        ).to(device)\n",
    "        model.load_state_dict(torch.load(fold_model_path)['model_state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval() # 평가 모드\n",
    "\n",
    "        if USE_TTA:\n",
    "            fold_preds_proba, img_names = inference_with_tta(\n",
    "                loader=test_loader,\n",
    "                model=model,\n",
    "                device=device,\n",
    "                tta_augments=tta_augments,\n",
    "                base_transform=tst_transform\n",
    "            )\n",
    "        else:\n",
    "            # TTA를 사용하지 않는 경우의 일반 추론\n",
    "            fold_preds_proba_list = []\n",
    "            img_names = []\n",
    "            with torch.no_grad():\n",
    "                pbar = tqdm(test_loader)\n",
    "                for image_batch, _, img_names_batch in pbar:\n",
    "                    image = image_batch.to(device)\n",
    "                    outputs = model(image)\n",
    "                    prob = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                    fold_preds_proba_list.extend(prob)\n",
    "                    img_names.extend(img_names_batch)\n",
    "            fold_preds_proba = np.array(fold_preds_proba_list)\n",
    "        \n",
    "        all_fold_predictions_proba.append(fold_preds_proba)\n",
    "        print(f\"Prediction for Fold {fold+1} completed.\", flush=True)\n",
    "\n",
    "    if not all_fold_predictions_proba:\n",
    "        print(\"모든 폴드의 모델 로드 또는 예측에 실패했습니다. 의사 레이블을 생성할 수 없습니다.\", flush=True)\n",
    "        wandb.finish()\n",
    "        exit()\n",
    "\n",
    "    # 3. 모든 폴드의 예측 확률 평균 (앙상블)\n",
    "    print(\"\\n--- Averaging predictions from all folds ---\", flush=True)\n",
    "    ensemble_preds_proba = np.mean(all_fold_predictions_proba, axis=0)\n",
    "    \n",
    "    # 4. 의사 레이블 선별\n",
    "    print(f\"Filtering pseudo-labels with confidence threshold: {CONFIDENCE_THRESHOLD}\", flush=True)\n",
    "    max_confidences = np.max(ensemble_preds_proba, axis=1)\n",
    "    pseudo_labels = np.argmax(ensemble_preds_proba, axis=1)\n",
    "\n",
    "    confident_indices = np.where(max_confidences >= CONFIDENCE_THRESHOLD)[0]\n",
    "    \n",
    "    # 의사 레이블링된 데이터프레임 생성\n",
    "    pseudo_labeled_df = pd.DataFrame({\n",
    "        'ID': [img_names[i] for i in confident_indices],\n",
    "        'target': [pseudo_labels[i] for i in confident_indices]\n",
    "    })\n",
    "    \n",
    "    print(f\"Total test samples: {len(test_dataset)}\", flush=True)\n",
    "    print(f\"Number of pseudo-labeled samples: {len(pseudo_labeled_df)}\", flush=True)\n",
    "\n",
    "    # 5. 기존 학습 데이터와 의사 레이블 데이터 결합\n",
    "    # print(\"Combining original training data with pseudo-labeled data...\", flush=True)\n",
    "    # original_train_df = pd.read_csv(train_csv_file)\n",
    "    # combined_df = pd.concat([original_train_df, pseudo_labeled_df], ignore_index=True)\n",
    "    \n",
    "    # 6. 새로운 CSV 파일로 저장\n",
    "    pseudo_labeled_df.to_csv(PSEUDO_LABEL_CSV_PATH, index=False)\n",
    "    print(f\"Pseudo-labeled dataset saved to {PSEUDO_LABEL_CSV_PATH}\", flush=True)\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during pseudo-labeling: {e}\", flush=True)\n",
    "    wandb.finish(exit_code=1) # 에러 발생 시 wandb run을 실패로 표시\n",
    "    raise # 에러를 다시 발생시켜 스크립트 종료\n",
    "finally:\n",
    "    print(\"=\" * 60, flush=True)\n",
    "    print(f\"🎉 Pseudo-labeling process completed! 🎉\", flush=True)\n",
    "    print(\"=\" * 60, flush=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
