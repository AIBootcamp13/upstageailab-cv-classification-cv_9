{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1648cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TTAì— í•„ìš”í•œ cv2 ì„í¬íŠ¸\n",
    "import cv2\n",
    "# Softmaxë¥¼ ìœ„í•œ torch.nn.functional ì„í¬íŠ¸\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Weights & Biases (wandb) ì„í¬íŠ¸\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cb3bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤. (ì¶”ë¡  ì‹œì—ë„ ì¬í˜„ì„±ì„ ìœ„í•´)\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d8ce8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. (training.pyì™€ ë™ì¼í•˜ê²Œ ìœ ì§€)\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, path, transform=None, return_raw=False):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.return_raw = return_raw\n",
    "\n",
    "        if 'ID' not in self.df.columns:\n",
    "            raise ValueError(f\"CSV íŒŒì¼ '{csv_file}'ì— 'ID' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        if 'target' not in self.df.columns:\n",
    "            # í…ŒìŠ¤íŠ¸ CSV íŒŒì¼ì€ 'target' ì»¬ëŸ¼ì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê²½ê³ ë§Œ í‘œì‹œ\n",
    "            print(f\"ê²½ê³ : CSV íŒŒì¼ '{csv_file}'ì— 'target' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ê°€ì •í•©ë‹ˆë‹¤.\", flush=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.loc[idx, 'ID']\n",
    "        # í…ŒìŠ¤íŠ¸ì…‹ì˜ ê²½ìš° 'target' ì»¬ëŸ¼ì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ 0 ì‚¬ìš©\n",
    "        target = self.df.loc[idx, 'target'] if 'target' in self.df.columns else 0 \n",
    "\n",
    "        img_path = os.path.join(self.path, img_name)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"ê²½ê³ : ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {img_path}. ì´ í•­ëª©ì€ ê±´ë„ˆë›°ê±°ë‚˜ ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\", flush=True)\n",
    "\n",
    "        if self.return_raw:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "            return img, target\n",
    "        else:\n",
    "            if self.transform:\n",
    "                if hasattr(self.transform, '__module__') and 'torchvision.transforms' in self.transform.__module__:\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    img = self.transform(img)\n",
    "                else:\n",
    "                    img = np.array(Image.open(img_path).convert('RGB'))\n",
    "                    img = self.transform(image=img)['image']\n",
    "            else:\n",
    "                img = np.array(Image.open(img_path).convert('RGB'))\n",
    "\n",
    "            return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TTAë¥¼ ìœ„í•œ ì¶”ë¡  í•¨ìˆ˜\n",
    "# def inference_with_tta(loader, model, device, tta_augments, base_transform):\n",
    "#     model.eval()\n",
    "#     all_preds_proba = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         pbar = tqdm(loader)\n",
    "#         for raw_image_batch, _ in pbar:\n",
    "#             raw_image = raw_image_batch.squeeze(0).cpu().numpy()\n",
    "\n",
    "#             single_image_tta_preds = []\n",
    "\n",
    "#             for tta_aug in tta_augments:\n",
    "#                 augmented_img_np = tta_aug(image=raw_image)['image']\n",
    "#                 transformed_img_tensor = base_transform(image=augmented_img_np)['image']\n",
    "#                 transformed_img_tensor = transformed_img_tensor.unsqueeze(0).to(device)\n",
    "#                 output = model(transformed_img_tensor)\n",
    "#                 prob = F.softmax(output, dim=1).cpu().numpy()\n",
    "#                 single_image_tta_preds.append(prob)\n",
    "\n",
    "#             avg_prob = np.mean(single_image_tta_preds, axis=0)\n",
    "#             all_preds_proba.append(avg_prob)\n",
    "\n",
    "#     final_preds_proba = np.concatenate(all_preds_proba, axis=0)\n",
    "\n",
    "#     return final_preds_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TTAë¥¼ ìœ„í•œ ì¶”ë¡  í•¨ìˆ˜\n",
    "def inference_with_tta(loader, model, device, tta_augments, base_transform):\n",
    "    model.eval()\n",
    "    all_preds_proba = []\n",
    "    all_img_names = [] # ì´ë¯¸ì§€ ì´ë¦„ì„ ì €ì¥\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader)\n",
    "        for raw_image_batch, img_names_batch in pbar: # ì´ë¯¸ì§€ ì´ë¦„ë„ ë°›ìŒ\n",
    "            raw_image = raw_image_batch.squeeze(0).cpu().numpy()\n",
    "            img_name = img_names_batch[0] # ë°°ì¹˜ í¬ê¸° 1ì´ë¯€ë¡œ ì²« ë²ˆì§¸ ìš”ì†Œ\n",
    "\n",
    "            single_image_tta_preds = []\n",
    "\n",
    "            for tta_aug in tta_augments:\n",
    "                augmented_img_np = tta_aug(image=raw_image)['image']\n",
    "                transformed_img_tensor = base_transform(image=augmented_img_np)['image']\n",
    "                transformed_img_tensor = transformed_img_tensor.unsqueeze(0).to(device)\n",
    "                output = model(transformed_img_tensor)\n",
    "                prob = F.softmax(output, dim=1).cpu().numpy()\n",
    "                single_image_tta_preds.append(prob)\n",
    "\n",
    "            avg_prob = np.mean(single_image_tta_preds, axis=0)\n",
    "            all_preds_proba.append(avg_prob)\n",
    "            all_img_names.append(img_name) # ì´ë¯¸ì§€ ì´ë¦„ ì €ì¥\n",
    "\n",
    "    final_preds_proba = np.concatenate(all_preds_proba, axis=0)\n",
    "\n",
    "    return final_preds_proba, all_img_names # í™•ë¥ ê³¼ ì´ë¯¸ì§€ ì´ë¦„ ë°˜í™˜\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa309d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Hyper-parameters (ì¶”ë¡ ì— í•„ìš”í•œ ìµœì†Œí•œì˜ íŒŒë¼ë¯¸í„°) ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "test_csv = \"data/sample_submission.csv\"\n",
    "test_path = \"data/test/\"\n",
    "checkpoint_dir = \"checkpoints\" # í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ëœ ê²½ë¡œ\n",
    "\n",
    "model_name = 'convnext_base' # í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ëª¨ë¸ ì´ë¦„ê³¼ ë™ì¼í•´ì•¼ í•¨\n",
    "img_size = 224 # í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ì´ë¯¸ì§€ í¬ê¸°ì™€ ë™ì¼í•´ì•¼ í•¨\n",
    "NUM_CLASSES = 17 # ë¶„ë¥˜í•  í´ë˜ìŠ¤ ê°œìˆ˜ (í•™ìŠµ ì‹œì™€ ë™ì¼í•´ì•¼ í•¨)\n",
    "BATCH_SIZE = 32 # ì¼ë°˜ ì¶”ë¡  ì‹œ ë°°ì¹˜ í¬ê¸°\n",
    "num_workers = 0 # DataLoader worker ìˆ˜\n",
    "\n",
    "USE_TTA = True # TTA ì‚¬ìš© ì—¬ë¶€ í”Œë˜ê·¸\n",
    "N_SPLITS = 5 # K-Fold í•™ìŠµ ì‹œ ì‚¬ìš©í•œ í´ë“œ ê°œìˆ˜ì™€ ë™ì¼í•´ì•¼ í•¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Weights & Biases ì´ˆê¸°í™” (ì¶”ë¡ ìš©) ---\n",
    "# wandb.init(project=\"document-type-classification-inference\",\n",
    "#            name=f\"inference_{model_name}_k{N_SPLITS}_tta{USE_TTA}\", # ì¶”ë¡  ì‹¤í–‰ ì´ë¦„\n",
    "#            config={\n",
    "#                \"model_name\": model_name,\n",
    "#                \"img_size\": img_size,\n",
    "#                \"num_classes\": NUM_CLASSES,\n",
    "#                \"use_tta\": USE_TTA,\n",
    "#                \"n_splits\": N_SPLITS, # K-Fold ì •ë³´ ì¶”ê°€\n",
    "#            },\n",
    "#            mode=\"online\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89795c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Data Transforms (ì¶”ë¡ ì— í•„ìš”í•œ ì „ì²˜ë¦¬) ---\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# TTA specific augmentations\n",
    "tta_augments = [\n",
    "    A.NoOp(),\n",
    "    A.HorizontalFlip(p=1),\n",
    "    A.VerticalFlip(p=1),\n",
    "    A.Rotate(limit=90, p=1, interpolation=cv2.INTER_LINEAR),\n",
    "    A.Rotate(limit=180, p=1, interpolation=cv2.INTER_LINEAR),\n",
    "    A.Rotate(limit=270, p=1, interpolation=cv2.INTER_LINEAR),\n",
    "    # A.RandomBrightnessContrast(p=1, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n",
    "    # A.RandomGamma(p=1, gamma_limit=(80, 120)),\n",
    "    # A.GaussNoise(p=1, var_limit=(10.0, 50.0)),\n",
    "    A.Sharpen(alpha=(0.3, 0.6), lightness=(0.7, 1.0), p=1.0),\n",
    "    A.CLAHE(p=1, clip_limit=2.0, tile_grid_size=(8,8)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95f4a1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting inference process...\n",
      "\n",
      "--- Loading model for Fold 1/5 ---\n",
      "Model loaded from checkpoints/best_model_fold_0.pth\n",
      "Performing inference with Test-Time Augmentation (TTA) for Fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3140/3140 [03:52<00:00, 13.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Fold 1 completed.\n",
      "\n",
      "--- Loading model for Fold 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoints/best_model_fold_1.pth\n",
      "Performing inference with Test-Time Augmentation (TTA) for Fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3140/3140 [03:53<00:00, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Fold 2 completed.\n",
      "\n",
      "--- Loading model for Fold 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoints/best_model_fold_2.pth\n",
      "Performing inference with Test-Time Augmentation (TTA) for Fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3140/3140 [03:52<00:00, 13.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Fold 3 completed.\n",
      "\n",
      "--- Loading model for Fold 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoints/best_model_fold_3.pth\n",
      "Performing inference with Test-Time Augmentation (TTA) for Fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3140/3140 [03:51<00:00, 13.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Fold 4 completed.\n",
      "\n",
      "--- Loading model for Fold 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoints/best_model_fold_4.pth\n",
      "Performing inference with Test-Time Augmentation (TTA) for Fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3140/3140 [03:51<00:00, 13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Fold 5 completed.\n",
      "\n",
      "--- Averaging predictions from all folds ---\n",
      "Final ensemble predictions saved to submission_kfold_ensemble_tta.csv\n",
      "============================================================\n",
      "ğŸ‰ All inference processes completed! ğŸ‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Inference & Save File ---\n",
    "print(\"ğŸš€ Starting inference process...\", flush=True)\n",
    "\n",
    "# ëª¨ë“  í´ë“œì˜ ì˜ˆì¸¡ í™•ë¥ ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "all_fold_predictions_proba = []\n",
    "\n",
    "# K-Fold ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "for fold in range(N_SPLITS):\n",
    "    fold_model_path = os.path.join(checkpoint_dir, f'best_model_fold_{fold}.pth')\n",
    "    \n",
    "    if not os.path.exists(fold_model_path):\n",
    "        print(f\"Error: Model for Fold {fold} not found at {fold_model_path}. Skipping this fold.\", flush=True)\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Loading model for Fold {fold+1}/{N_SPLITS} ---\", flush=True)\n",
    "    model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=False,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(device)\n",
    "    model.load_state_dict(torch.load(fold_model_path)['model_state_dict'])\n",
    "    model.to(device)\n",
    "    print(f\"Model loaded from {fold_model_path}\", flush=True)\n",
    "\n",
    "    if USE_TTA:\n",
    "        print(f\"Performing inference with Test-Time Augmentation (TTA) for Fold {fold+1}...\", flush=True)\n",
    "        tta_test_dataset = ImageDataset(csv_file=test_csv, path=test_path, return_raw=True)\n",
    "        tta_test_loader = DataLoader(tta_test_dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        fold_preds_proba = inference_with_tta(\n",
    "            loader=tta_test_loader,\n",
    "            model=model,\n",
    "            device=device,\n",
    "            tta_augments=tta_augments,\n",
    "            base_transform=tst_transform\n",
    "        )\n",
    "        all_fold_predictions_proba.append(fold_preds_proba)\n",
    "        print(f\"Inference for Fold {fold+1} completed.\", flush=True)\n",
    "\n",
    "    else:\n",
    "        print(f\"Performing normal inference for Fold {fold+1}...\", flush=True)\n",
    "        test_dataset_normal_inference = ImageDataset(\n",
    "            csv_file=test_csv,\n",
    "            path=test_path,\n",
    "            transform=tst_transform,\n",
    "            return_raw=False\n",
    "        )\n",
    "        test_loader_normal = DataLoader(\n",
    "            test_dataset_normal_inference,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        fold_preds_proba_list = []\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(test_loader_normal)\n",
    "            for image, _ in pbar:\n",
    "                image = image.to(device)\n",
    "                outputs = model(image)\n",
    "                prob = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                fold_preds_proba_list.extend(prob)\n",
    "        all_fold_predictions_proba.append(np.array(fold_preds_proba_list))\n",
    "        print(f\"Inference for Fold {fold+1} completed.\", flush=True)\n",
    "\n",
    "# ëª¨ë“  í´ë“œì˜ ì˜ˆì¸¡ í™•ë¥  í‰ê· \n",
    "if all_fold_predictions_proba:\n",
    "    print(\"\\n--- Averaging predictions from all folds ---\", flush=True)\n",
    "    final_avg_preds_proba = np.mean(all_fold_predictions_proba, axis=0)\n",
    "    final_preds = np.argmax(final_avg_preds_proba, axis=1)\n",
    "\n",
    "    submission_df = pd.read_csv(test_csv)\n",
    "    submission_file_name = 'submission_kfold_ensemble_tta.csv' if USE_TTA else 'submission_kfold_ensemble_normal.csv'\n",
    "    submission_df['target'] = final_preds\n",
    "    submission_df.to_csv(submission_file_name, index=False)\n",
    "    print(f\"Final ensemble predictions saved to {submission_file_name}\", flush=True)\n",
    "\n",
    "    # --- Weights & Biasesì— ì œì¶œ íŒŒì¼ ì•„í‹°íŒ©íŠ¸ë¡œ ê¸°ë¡ ---\n",
    "    # artifact = wandb.Artifact(f'submission_ensemble_tta_{N_SPLITS}folds' if USE_TTA else f'submission_ensemble_normal_{N_SPLITS}folds', type='submission')\n",
    "    # artifact.add_file(submission_file_name)\n",
    "    # wandb.log_artifact(artifact)\n",
    "    # wandb.log({\"final_submission_file\": submission_file_name})\n",
    "\n",
    "else:\n",
    "    print(\"No models were successfully loaded or processed for inference. No submission file generated.\", flush=True)\n",
    "\n",
    "print(\"=\" * 60, flush=True)\n",
    "print(f\"ğŸ‰ All inference processes completed! ğŸ‰\", flush=True)\n",
    "\n",
    "# --- Weights & Biases ì‹¤í–‰ ì¢…ë£Œ ---\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b94f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo labeling ì„ ìœ„í•œ inference ì½”ë“œ\n",
    "\n",
    "# TTAë¥¼ ìœ„í•œ ì¶”ë¡  í•¨ìˆ˜\n",
    "def inference_with_tta(loader, model, device, tta_augments, base_transform):\n",
    "    model.eval()\n",
    "    all_preds_proba = []\n",
    "    all_img_names = [] # ì´ë¯¸ì§€ ì´ë¦„ì„ ì €ì¥\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader)\n",
    "        for raw_image_batch, img_names_batch in pbar: # ì´ë¯¸ì§€ ì´ë¦„ë„ ë°›ìŒ\n",
    "            raw_image = raw_image_batch.squeeze(0).cpu().numpy()\n",
    "            img_name = img_names_batch[0] # ë°°ì¹˜ í¬ê¸° 1ì´ë¯€ë¡œ ì²« ë²ˆì§¸ ìš”ì†Œ\n",
    "\n",
    "            single_image_tta_preds = []\n",
    "\n",
    "            for tta_aug in tta_augments:\n",
    "                augmented_img_np = tta_aug(image=raw_image)['image']\n",
    "                transformed_img_tensor = base_transform(image=augmented_img_np)['image']\n",
    "                transformed_img_tensor = transformed_img_tensor.unsqueeze(0).to(device)\n",
    "                output = model(transformed_img_tensor)\n",
    "                prob = F.softmax(output, dim=1).cpu().numpy()\n",
    "                single_image_tta_preds.append(prob)\n",
    "\n",
    "            avg_prob = np.mean(single_image_tta_preds, axis=0)\n",
    "            all_preds_proba.append(avg_prob)\n",
    "            all_img_names.append(img_name) # ì´ë¯¸ì§€ ì´ë¦„ ì €ì¥\n",
    "\n",
    "    final_preds_proba = np.concatenate(all_preds_proba, axis=0)\n",
    "\n",
    "    return final_preds_proba, all_img_names # í™•ë¥ ê³¼ ì´ë¯¸ì§€ ì´ë¦„ ë°˜í™˜\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef61431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Hyper-parameters (ì¶”ë¡ ì— í•„ìš”í•œ ìµœì†Œí•œì˜ íŒŒë¼ë¯¸í„°) ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "test_csv = \"data/sample_submission.csv\"\n",
    "test_path = \"data/test/\"\n",
    "# checkpoint_dir = \"checkpoints_0707_kfold_conv\" # í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ëœ ê²½ë¡œ\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "\n",
    "model_name = 'convnext_base' # í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ëª¨ë¸ ì´ë¦„ê³¼ ë™ì¼í•´ì•¼ í•¨\n",
    "img_size = 224 # í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ì´ë¯¸ì§€ í¬ê¸°ì™€ ë™ì¼í•´ì•¼ í•¨\n",
    "NUM_CLASSES = 17 # ë¶„ë¥˜í•  í´ë˜ìŠ¤ ê°œìˆ˜ (í•™ìŠµ ì‹œì™€ ë™ì¼í•´ì•¼ í•¨)\n",
    "BATCH_SIZE = 32 # ì¼ë°˜ ì¶”ë¡  ì‹œ ë°°ì¹˜ í¬ê¸°\n",
    "num_workers = 0 # DataLoader worker ìˆ˜\n",
    "\n",
    "USE_TTA = True # TTA ì‚¬ìš© ì—¬ë¶€ í”Œë˜ê·¸\n",
    "N_SPLITS = 5 # K-Fold í•™ìŠµ ì‹œ ì‚¬ìš©í•œ í´ë“œ ê°œìˆ˜ì™€ ë™ì¼í•´ì•¼ í•¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f93017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pseudo-labeling specific parameters\n",
    "CONFIDENCE_THRESHOLD = 0.9 # ì˜ì‚¬ ë ˆì´ë¸”ì„ ë¶€ì—¬í•  í™•ì‹ ë„ ì„ê³„ê°’\n",
    "PSEUDO_LABEL_CSV_PATH = \"train_pseudo_labeled.csv\" # ì˜ì‚¬ ë ˆì´ë¸”ë§ëœ ë°ì´í„° ì €ì¥ ê²½ë¡œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfa31646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting pseudo-labeling process...\n",
      "Loading 5 models and predicting on test data...\n",
      "--- Predicting with model from Fold 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3140/3140 [05:36<00:00,  9.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Fold 1 completed.\n",
      "--- Predicting with model from Fold 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3140/3140 [05:36<00:00,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Fold 2 completed.\n",
      "--- Predicting with model from Fold 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3140/3140 [05:38<00:00,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Fold 3 completed.\n",
      "--- Predicting with model from Fold 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3140/3140 [05:38<00:00,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Fold 4 completed.\n",
      "--- Predicting with model from Fold 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3140/3140 [05:42<00:00,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Fold 5 completed.\n",
      "\n",
      "--- Averaging predictions from all folds ---\n",
      "Filtering pseudo-labels with confidence threshold: 0.9\n",
      "Total test samples: 3140\n",
      "Number of pseudo-labeled samples: 1774\n",
      "Pseudo-labeled dataset saved to train_pseudo_labeled.csv\n",
      "============================================================\n",
      "ğŸ‰ Pseudo-labeling process completed! ğŸ‰\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Data Transforms (ì¶”ë¡ ì— í•„ìš”í•œ ì „ì²˜ë¦¬) ---\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# TTA specific augmentations\n",
    "tta_augments = [\n",
    "    A.NoOp(),\n",
    "    A.HorizontalFlip(p=1),\n",
    "    A.VerticalFlip(p=1),\n",
    "    A.Rotate(limit=90, p=1, interpolation=cv2.INTER_LINEAR),\n",
    "    A.Rotate(limit=180, p=1, interpolation=cv2.INTER_LINEAR),\n",
    "    A.Rotate(limit=270, p=1, interpolation=cv2.INTER_LINEAR),\n",
    "    A.RandomBrightnessContrast(p=1, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n",
    "    A.RandomGamma(p=1, gamma_limit=(80, 120)),\n",
    "    A.GaussNoise(p=1, var_limit=(10.0, 50.0)),\n",
    "    A.CLAHE(p=1, clip_limit=2.0, tile_grid_size=(8,8)),\n",
    "]\n",
    "\n",
    "# --- Main Pseudo-Labeling Process ---\n",
    "try:\n",
    "    print(\"ğŸš€ Starting pseudo-labeling process...\", flush=True)\n",
    "\n",
    "    # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "    # ImageDatasetì—ì„œ ì´ë¯¸ì§€ ì´ë¦„ë„ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •í–ˆìœ¼ë¯€ë¡œ, return_raw=Trueë¥¼ ì‚¬ìš©\n",
    "    test_dataset = ImageDataset(csv_file=test_csv, path=test_path, return_raw=True)\n",
    "    # TTAë¥¼ ìœ„í•´ batch_size=1ë¡œ ì„¤ì •\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # ëª¨ë“  í´ë“œì˜ ì˜ˆì¸¡ í™•ë¥ ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    all_fold_predictions_proba = []\n",
    "\n",
    "    # 2. K-Fold ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "    print(f\"Loading {N_SPLITS} models and predicting on test data...\", flush=True)\n",
    "    for fold in range(N_SPLITS):\n",
    "        fold_model_path = os.path.join(checkpoint_dir, f'best_model_fold_{fold}.pth')\n",
    "        \n",
    "        if not os.path.exists(fold_model_path):\n",
    "            print(f\"ê²½ê³ : Fold {fold}ì˜ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {fold_model_path}. ì´ í´ë“œëŠ” ê±´ë„ˆëœë‹ˆë‹¤.\", flush=True)\n",
    "            continue\n",
    "\n",
    "        print(f\"--- Predicting with model from Fold {fold+1}/{N_SPLITS} ---\", flush=True)\n",
    "        model = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=False,\n",
    "            num_classes=NUM_CLASSES\n",
    "        ).to(device)\n",
    "        model.load_state_dict(torch.load(fold_model_path)['model_state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval() # í‰ê°€ ëª¨ë“œ\n",
    "\n",
    "        if USE_TTA:\n",
    "            fold_preds_proba, img_names = inference_with_tta(\n",
    "                loader=test_loader,\n",
    "                model=model,\n",
    "                device=device,\n",
    "                tta_augments=tta_augments,\n",
    "                base_transform=tst_transform\n",
    "            )\n",
    "        else:\n",
    "            # TTAë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì˜ ì¼ë°˜ ì¶”ë¡ \n",
    "            fold_preds_proba_list = []\n",
    "            img_names = []\n",
    "            with torch.no_grad():\n",
    "                pbar = tqdm(test_loader)\n",
    "                for image_batch, _, img_names_batch in pbar:\n",
    "                    image = image_batch.to(device)\n",
    "                    outputs = model(image)\n",
    "                    prob = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                    fold_preds_proba_list.extend(prob)\n",
    "                    img_names.extend(img_names_batch)\n",
    "            fold_preds_proba = np.array(fold_preds_proba_list)\n",
    "        \n",
    "        all_fold_predictions_proba.append(fold_preds_proba)\n",
    "        print(f\"Prediction for Fold {fold+1} completed.\", flush=True)\n",
    "\n",
    "    if not all_fold_predictions_proba:\n",
    "        print(\"ëª¨ë“  í´ë“œì˜ ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ì˜ˆì¸¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜ì‚¬ ë ˆì´ë¸”ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\", flush=True)\n",
    "        wandb.finish()\n",
    "        exit()\n",
    "\n",
    "    # 3. ëª¨ë“  í´ë“œì˜ ì˜ˆì¸¡ í™•ë¥  í‰ê·  (ì•™ìƒë¸”)\n",
    "    print(\"\\n--- Averaging predictions from all folds ---\", flush=True)\n",
    "    ensemble_preds_proba = np.mean(all_fold_predictions_proba, axis=0)\n",
    "    \n",
    "    # 4. ì˜ì‚¬ ë ˆì´ë¸” ì„ ë³„\n",
    "    print(f\"Filtering pseudo-labels with confidence threshold: {CONFIDENCE_THRESHOLD}\", flush=True)\n",
    "    max_confidences = np.max(ensemble_preds_proba, axis=1)\n",
    "    pseudo_labels = np.argmax(ensemble_preds_proba, axis=1)\n",
    "\n",
    "    confident_indices = np.where(max_confidences >= CONFIDENCE_THRESHOLD)[0]\n",
    "    \n",
    "    # ì˜ì‚¬ ë ˆì´ë¸”ë§ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "    pseudo_labeled_df = pd.DataFrame({\n",
    "        'ID': [img_names[i] for i in confident_indices],\n",
    "        'target': [pseudo_labels[i] for i in confident_indices]\n",
    "    })\n",
    "    \n",
    "    print(f\"Total test samples: {len(test_dataset)}\", flush=True)\n",
    "    print(f\"Number of pseudo-labeled samples: {len(pseudo_labeled_df)}\", flush=True)\n",
    "\n",
    "    # 5. ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ì™€ ì˜ì‚¬ ë ˆì´ë¸” ë°ì´í„° ê²°í•©\n",
    "    # print(\"Combining original training data with pseudo-labeled data...\", flush=True)\n",
    "    # original_train_df = pd.read_csv(train_csv_file)\n",
    "    # combined_df = pd.concat([original_train_df, pseudo_labeled_df], ignore_index=True)\n",
    "    \n",
    "    # 6. ìƒˆë¡œìš´ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "    pseudo_labeled_df.to_csv(PSEUDO_LABEL_CSV_PATH, index=False)\n",
    "    print(f\"Pseudo-labeled dataset saved to {PSEUDO_LABEL_CSV_PATH}\", flush=True)\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during pseudo-labeling: {e}\", flush=True)\n",
    "    wandb.finish(exit_code=1) # ì—ëŸ¬ ë°œìƒ ì‹œ wandb runì„ ì‹¤íŒ¨ë¡œ í‘œì‹œ\n",
    "    raise # ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ\n",
    "finally:\n",
    "    print(\"=\" * 60, flush=True)\n",
    "    print(f\"ğŸ‰ Pseudo-labeling process completed! ğŸ‰\", flush=True)\n",
    "    print(\"=\" * 60, flush=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
